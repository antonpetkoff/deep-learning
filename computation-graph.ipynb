{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    \"\"\"Represents a Node in the Computation Graph\"\"\"\n",
    "    \n",
    "    def __init__(self, input_nodes = []):\n",
    "        \"\"\"Constructs an Operation with input_nodes as inputs\n",
    "           which computes outputs to zero or more consumers\"\"\"\n",
    "        self.input_nodes = input_nodes\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Connect this node with its inputs, by adding it as a consumer to its inputs\n",
    "        for input_node in self.input_nodes:\n",
    "            input_node.consumers.append(self)\n",
    "        \n",
    "        # Add this operation to the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.operations.append(self)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Computes the output of the operation. Depends on the specific operation.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(input_nodes=[x, y])\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matmul(Operation):\n",
    "    def __init__(self, A, B):\n",
    "        super().__init__(input_nodes=[A, B])\n",
    "\n",
    "    def compute(self, A, B):\n",
    "        return A.dot(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        \"\"\"The input of Softmax is a vector\"\"\"\n",
    "        # using vector operations\n",
    "        # axis=1 so that for each row we sum its colums\n",
    "        # the sum will eat 1 dimension, so we broadcast with [:, None]\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        return np.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(Operation):\n",
    "    \"\"\"Element-wise multiplication of 2 matrices A and B\"\"\"\n",
    "    \n",
    "    def __init__(self, A, B):\n",
    "        super().__init__(input_nodes=[A, B])\n",
    "\n",
    "    def compute(self, A, B):\n",
    "        return A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceSum(Operation):\n",
    "    \"\"\"Computes the sum of the given tensor A based on the given axis.\n",
    "       axis=None computes the sum of the whole tensor A.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A, axis=None):\n",
    "        super().__init__(input_nodes=[A])\n",
    "        self.axis = axis\n",
    "\n",
    "    def compute(self, A):\n",
    "        return np.sum(A, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Negate(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "\n",
    "    def compute(self, x):\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder:\n",
    "    \"\"\"Represents an input node which doesn't have any inputs\n",
    "       and can only be consumed by other Nodes in the Computation Graph.\n",
    "       \n",
    "       The Placeholder has a fixed value. Acts like a constant.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Register the placeholder in the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.placeholders.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"Represents a parameter in the Computation Graph.\n",
    "       This node doesn't have any inputs and has only consumers.\n",
    "       \n",
    "       The Variable's value can change. It is initialized to initial_value.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_value=None):\n",
    "        self.value = initial_value\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Register the variable in the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.variables.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"Represents the actual Computation Graph which has 3 types of Nodes:\n",
    "       - placeholders\n",
    "       - variables\n",
    "       - operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, placeholders=[], variables=[], operations=[]):\n",
    "        self.placeholders = placeholders\n",
    "        self.variables = variables\n",
    "        self.operations = operations\n",
    "\n",
    "    def as_default(self):\n",
    "        global _default_graph\n",
    "        _default_graph = self\n",
    "        return _default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Session:\n",
    "    \"\"\"Represents a single execution of the whole Computation graph.\"\"\"\n",
    "    # TODO: provide the Graph explicitly\n",
    "    \n",
    "    def run(self, operation, feed_dict={}):\n",
    "        \"\"\"Performs a post-order traversal of all nodes in the Computation graph,\n",
    "           so that all operations with known inputs are performed first.\n",
    "        \"\"\"\n",
    "        \n",
    "        nodes_in_post_order = Session.traverse_post_order(operation)\n",
    "        \n",
    "        outputs = {operation: None for operation in nodes_in_post_order}\n",
    "        \n",
    "        for node in nodes_in_post_order:\n",
    "            if type(node) == Placeholder:\n",
    "                outputs[node] = feed_dict[node]\n",
    "            elif type(node) == Variable:\n",
    "                outputs[node] = node.value\n",
    "            elif isinstance(node, Operation):\n",
    "                computed_inputs = [outputs[input_node] for input_node in node.input_nodes]\n",
    "                outputs[node] = node.compute(*computed_inputs)\n",
    "\n",
    "        return outputs[operation]\n",
    "\n",
    "    @staticmethod\n",
    "    def traverse_post_order(operation):\n",
    "        operations_post_order = []\n",
    "        \n",
    "        def traverse(node):\n",
    "            # Placeholders and Variables do not have input_nodes\n",
    "            if isinstance(node, Operation):\n",
    "                for input_node in node.input_nodes:\n",
    "                    traverse(input_node)\n",
    "\n",
    "            operations_post_order.append(node)\n",
    "        \n",
    "        traverse(operation)\n",
    "        return operations_post_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "A = Variable(np.array([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "]))\n",
    "b = Variable(np.array([1, 1]))\n",
    "\n",
    "x = Placeholder()\n",
    "\n",
    "y = Add(Matmul(A, x), b)\n",
    "\n",
    "Session().run(y, feed_dict={\n",
    "    x: np.array([1, 2])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17487770808997996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "x = Placeholder()\n",
    "w = Variable(initial_value=np.random.normal(0, 1, 2))\n",
    "b = Variable(initial_value=np.random.normal(0, 1))\n",
    "\n",
    "perceptron = Sigmoid(Add(Matmul(w, x), b))\n",
    "\n",
    "Session().run(perceptron, feed_dict={\n",
    "    x: np.array([-1, 1])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.14417460e-06, 9.99993856e-01],\n",
       "       [8.31528028e-07, 9.99999168e-01],\n",
       "       [9.99999985e-01, 1.52299795e-08],\n",
       "       [9.99999985e-01, 1.52299795e-08]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-class Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "training_examples = np.array([\n",
    "    [-3, -3],\n",
    "    [-3, -4],\n",
    "    [4, 5],\n",
    "    [3, 6]\n",
    "])\n",
    "\n",
    "labels = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "# will be a matrix used for batch computation\n",
    "X = Placeholder()\n",
    "\n",
    "W = Variable(np.array([\n",
    "    [1, -1],\n",
    "    [1, -1]\n",
    "]))\n",
    "\n",
    "b = Variable(np.array([0, 0]))\n",
    "\n",
    "classifier = Softmax(Add(Matmul(X, W), b))\n",
    "\n",
    "Session().run(classifier, {\n",
    "    X: training_examples\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.006181810554592e-06"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-entropy loss\n",
    "\n",
    "C = Placeholder()\n",
    "\n",
    "# TODO: not sure about the ReduceSum over all dimensions here\n",
    "cross_entropy_loss = Negate(ReduceSum(\n",
    "    Multiply(C, Log(classifier))\n",
    "))\n",
    "\n",
    "Session().run(cross_entropy_loss, {\n",
    "    X: training_examples,\n",
    "    C: labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Optimizer\n",
    "\n",
    "class GradientDescentOptimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def minimize(self, loss):\n",
    "        learning_rate = self.learning_rate\n",
    "\n",
    "        class Minimize(Operation):\n",
    "            def compute(self, loss):\n",
    "                # TODO: how to inject compute_gradients or just the gradients at each step?\n",
    "                gradients_table = compute_gradients(loss)\n",
    "\n",
    "                for node, gradient in gradients_table.items():\n",
    "                    if type(node) == Variable:\n",
    "                        node.value -= learning_rate * gradient\n",
    "        \n",
    "        return Minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards pass of Backpropagation which computes the gradients\n",
    "\n",
    "from queue import Queue\n",
    "\n",
    "def compute_gradients(loss):\n",
    "    gradients_table = {}\n",
    "    \n",
    "    # the loss gradient with regard to itself is 1\n",
    "    gradients_table[loss] = 1\n",
    "    \n",
    "    visited = set()\n",
    "    queue = Queue()\n",
    "    queue.put(loss)\n",
    "    visited.add(node)\n",
    "    \n",
    "    # BFS from loss backwards to inputs\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "\n",
    "#         if hasattr(node, 'consumers'):\n",
    "        if node != loss: # the loss doesn't have any consumers\n",
    "            # compute gradient of node\n",
    "\n",
    "            gradients_table[node] = 0\n",
    "            \n",
    "            for consumer in node.consumers:\n",
    "                # get the accumulated gradient for the consumer\n",
    "                loss_gradient_wrt_consumer_output = gradients_table[consumer]\n",
    "                \n",
    "                # TODO: use multiplication for the chain rule and do not pass gradients around\n",
    "                \n",
    "                # apply the chain rule, using consumer's accumulated gradient, to compute...\n",
    "                loss_gradient_wrt_consumer_inputs = node.gradient(loss_gradient_wrt_consumer_output)\n",
    "                \n",
    "                if len(consumer.input_nodes) == 1:\n",
    "                    # the consumer has only 1 input (the current node), thus consumer's input is scalar\n",
    "                    gradients_table[node] += loss_gradient_wrt_consumer_inputs\n",
    "                else:\n",
    "                    # otherwise the consumer consumes a vector\n",
    "                    # find the index of node in consumer's inputs\n",
    "                    node_index_in_consumer_inputs = consumer.input_nodes.index(node)\n",
    "                    \n",
    "                    # get the gradient only from the edge from node to consumer\n",
    "                    loss_gradient_wrt_node = loss_gradient_wrt_consumer_inputs[node_index_in_consumer_inputs]\n",
    "                    \n",
    "                    # accumulate the gradient for that consumer\n",
    "                    gradients_table[node] += loss_gradient_wrt_node\n",
    "\n",
    "        # continue backwards to node's inputs\n",
    "        if hasattr(node, 'input_nodes'):\n",
    "            for input_node in node.input_nodes:\n",
    "                if not visited.get(input_node):\n",
    "                    queue.put(input_node)\n",
    "                    visited.add(input_node)\n",
    "    \n",
    "    return gradients_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
