{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    \"\"\"Represents a Node in the Computation Graph\"\"\"\n",
    "    \n",
    "    def __init__(self, input_nodes = []):\n",
    "        \"\"\"Constructs an Operation with input_nodes as inputs\n",
    "           which computes outputs to zero or more consumers\"\"\"\n",
    "        self.input_nodes = input_nodes\n",
    "        self.consumers = []\n",
    "        self.inputs = [] # evaluated after each forward pass, TODO: np.array?\n",
    "        self.output = None # evaluated after each forward pass\n",
    "        \n",
    "        # Connect this node with its inputs, by adding it as a consumer to its inputs\n",
    "        for input_node in self.input_nodes:\n",
    "            input_node.consumers.append(self)\n",
    "        \n",
    "        # Add this operation to the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.operations.append(self)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Computes the output of the operation. Depends on the specific operation.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(input_nodes=[x, y])\n",
    "    \n",
    "    def compute(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        a = self.inputs[0]\n",
    "        b = self.inputs[1]\n",
    "\n",
    "        # TODO: can't really understand why we do all this summation\n",
    "        grad_wrt_a = accum_gradient        \n",
    "        while np.ndim(grad_wrt_a) > len(a.shape):\n",
    "            grad_wrt_a = np.sum(grad_wrt_a, axis=0)\n",
    "        for axis, size in enumerate(a.shape):\n",
    "            if size == 1:\n",
    "                grad_wrt_a = np.sum(grad_wrt_a, axis=axis, keepdims=True)\n",
    "\n",
    "        grad_wrt_b = accum_gradient\n",
    "        while np.ndim(grad_wrt_b) > len(b.shape):\n",
    "            grad_wrt_b = np.sum(grad_wrt_b, axis=0)\n",
    "        for axis, size in enumerate(b.shape):\n",
    "            if size == 1:\n",
    "                grad_wrt_b = np.sum(grad_wrt_b, axis=axis, keepdims=True)\n",
    "\n",
    "        return [grad_wrt_a, grad_wrt_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matmul(Operation):\n",
    "    def __init__(self, A, B):\n",
    "        super().__init__(input_nodes=[A, B])\n",
    "\n",
    "    def compute(self, A, B):\n",
    "        return A.dot(B)\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        A = self.inputs[0]\n",
    "        B = self.inputs[1]\n",
    "        # Because of gradients like this we cannot simply\n",
    "        # apply the chain rule by multiplying by the accumulated gradient.\n",
    "        # For the transpose matrices below, see Goodfellow's Deep Learning book, page 216\n",
    "        return [accum_gradient.dot(B.T), A.T.dot(accum_gradient)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        return self.output * (1 - self.output) * accum_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        \"\"\"The input of Softmax is a vector\"\"\"\n",
    "        # using vector operations\n",
    "        # axis=1 so that for each row we sum its colums\n",
    "        # the sum will eat 1 dimension, so we broadcast with [:, None]\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        softmax = self.output\n",
    "        \n",
    "        # TODO: can't really understand why this is correct\n",
    "        return (accum_gradient - np.reshape(\n",
    "            np.sum(accum_gradient * softmax, 1),\n",
    "            [-1, 1]\n",
    "        )) * softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax playground\n",
    "softmax = np.array([0.3, 0.2, 0.4, 0.1])\n",
    "\n",
    "upstream_grad = np.array([0.1, 0.1, 0.1, 0.1])\n",
    "# np.sum(upstream_grad * softmax.T, 1)\n",
    "np.reshape([1, 2], [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "    \n",
    "    def compute(self, x):\n",
    "        return np.log(x)\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        x = self.inputs[0]\n",
    "        return (1 / x) * accum_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(Operation):\n",
    "    \"\"\"Element-wise multiplication of 2 matrices A and B\"\"\"\n",
    "    \n",
    "    def __init__(self, A, B):\n",
    "        super().__init__(input_nodes=[A, B])\n",
    "\n",
    "    def compute(self, A, B):\n",
    "        return A * B\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        A = self.inputs[0]\n",
    "        B = self.inputs[1]\n",
    "        \n",
    "        # dimensionality of the gradient must match the inputs\n",
    "        # derivative of multiplication wrt A is B and wrt to B is A\n",
    "        return np.array([B, A]) * accum_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceSum(Operation):\n",
    "    \"\"\"Computes the sum of the given tensor A based on the given axis.\n",
    "       axis=None computes the sum of the whole tensor A.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, A, axis=None):\n",
    "        super().__init__(input_nodes=[A])\n",
    "        self.axis = axis\n",
    "\n",
    "    def compute(self, A):\n",
    "        return np.sum(A, axis=self.axis)\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        A = self.inputs[0]\n",
    "        \n",
    "        output_shape = np.array(A.shape)\n",
    "\n",
    "        # mark the dimension which was squashed by the sum in the forward pass\n",
    "        output_shape[self.axis] = 1\n",
    "        \n",
    "        # set all dimensions to 1, except the summing dimenion(s)\n",
    "        tile_scaling = A.shape // output_shape\n",
    "        \n",
    "        # restore the dimension(s), destroyed by the sum from the forward pass\n",
    "        upstream_grad = np.reshape(accum_gradient, output_shape)\n",
    "        \n",
    "        # copy the upstream gradient over the dimension(s) which was squashed by the sum\n",
    "        # so that it matches the dimensions of the given input\n",
    "        return np.tile(upstream_grad, tile_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06, 0.06, 0.06],\n",
       "       [0.12, 0.12, 0.12]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playground\n",
    "\n",
    "A = np.array([[1, 2, 3], [3, 4, 5]])\n",
    "axis = 1\n",
    "output = np.sum(A, axis)\n",
    "grad = output / 100\n",
    "\n",
    "output_shape = np.array(A.shape)\n",
    "output_shape[axis] = 1\n",
    "tile_scaling = A.shape // output_shape\n",
    "# np.tile(A, tile_scaling)\n",
    "upstream_grad = np.reshape(grad, output_shape)\n",
    "\n",
    "np.tile(upstream_grad, tile_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Negate(Operation):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(input_nodes=[x])\n",
    "\n",
    "    def compute(self, x):\n",
    "        return -x\n",
    "    \n",
    "    def gradient(self, accum_gradient):\n",
    "        return -1 * accum_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder:\n",
    "    \"\"\"Represents an input node which doesn't have any inputs\n",
    "       and can only be consumed by other Nodes in the Computation Graph.\n",
    "       \n",
    "       The Placeholder has a fixed value. Acts like a constant.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Register the placeholder in the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.placeholders.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"Represents a parameter in the Computation Graph.\n",
    "       This node doesn't have any inputs and has only consumers.\n",
    "       \n",
    "       The Variable's value can change. It is initialized to initial_value.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_value=None):\n",
    "        self.value = initial_value\n",
    "        self.consumers = []\n",
    "        \n",
    "        # Register the variable in the Computation Graph\n",
    "        # TODO: provide the graph explicitly\n",
    "        _default_graph.variables.append(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"Represents the actual Computation Graph which has 3 types of Nodes:\n",
    "       - placeholders\n",
    "       - variables\n",
    "       - operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, placeholders=[], variables=[], operations=[]):\n",
    "        self.placeholders = placeholders\n",
    "        self.variables = variables\n",
    "        self.operations = operations\n",
    "\n",
    "    def as_default(self):\n",
    "        global _default_graph\n",
    "        _default_graph = self\n",
    "        return _default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Session:\n",
    "    \"\"\"Represents a single execution of the whole Computation graph.\"\"\"\n",
    "    # TODO: provide the Graph explicitly\n",
    "    \n",
    "    def run(self, operation, feed_dict={}):\n",
    "        \"\"\"Performs a post-order traversal of all nodes in the Computation graph,\n",
    "           so that all operations with known inputs are performed first.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: cache this order\n",
    "        nodes_in_post_order = Session.traverse_post_order(operation)\n",
    "        \n",
    "        # TODO: use some kind of environment to store evaluated inputs and outputs of nodes\n",
    "        outputs = {operation: None for operation in nodes_in_post_order}\n",
    "        \n",
    "        for node in nodes_in_post_order:\n",
    "            if type(node) == Placeholder:\n",
    "                outputs[node] = feed_dict[node]\n",
    "            elif type(node) == Variable:\n",
    "                outputs[node] = node.value\n",
    "            elif isinstance(node, Operation):\n",
    "                computed_inputs = [outputs[input_node] for input_node in node.input_nodes]\n",
    "                node.inputs = computed_inputs\n",
    "                \n",
    "                outputs[node] = node.compute(*computed_inputs)\n",
    "                node.output = outputs[node]\n",
    "                \n",
    "        return outputs[operation]\n",
    "\n",
    "    @staticmethod\n",
    "    def traverse_post_order(operation):\n",
    "        operations_post_order = []\n",
    "        \n",
    "        def traverse(node):\n",
    "            # Placeholders and Variables do not have input_nodes\n",
    "            if isinstance(node, Operation):\n",
    "                for input_node in node.input_nodes:\n",
    "                    traverse(input_node)\n",
    "\n",
    "            operations_post_order.append(node)\n",
    "        \n",
    "        traverse(operation)\n",
    "        return operations_post_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "A = Variable(np.array([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "]))\n",
    "b = Variable(np.array([1, 1]))\n",
    "\n",
    "x = Placeholder()\n",
    "\n",
    "y = Add(Matmul(A, x), b)\n",
    "\n",
    "Session().run(y, feed_dict={\n",
    "    x: np.array([1, 2])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038057472201464576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "x = Placeholder()\n",
    "w = Variable(initial_value=np.random.normal(0, 1, 2))\n",
    "b = Variable(initial_value=np.random.normal(0, 1))\n",
    "\n",
    "perceptron = Sigmoid(Add(Matmul(w, x), b))\n",
    "\n",
    "Session().run(perceptron, feed_dict={\n",
    "    x: np.array([-1, 1])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.14417460e-06, 9.99993856e-01],\n",
       "       [8.31528028e-07, 9.99999168e-01],\n",
       "       [9.99999985e-01, 1.52299795e-08],\n",
       "       [9.99999985e-01, 1.52299795e-08]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-class Perceptron\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "training_examples = np.array([\n",
    "    [-3, -3],\n",
    "    [-3, -4],\n",
    "    [4, 5],\n",
    "    [3, 6]\n",
    "])\n",
    "\n",
    "labels = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "# will be a matrix used for batch computation\n",
    "X = Placeholder()\n",
    "\n",
    "W = Variable(np.array([\n",
    "    [1, -1],\n",
    "    [1, -1]\n",
    "]))\n",
    "\n",
    "b = Variable(np.array([0, 0]))\n",
    "\n",
    "classifier = Softmax(Add(Matmul(X, W), b))\n",
    "\n",
    "Session().run(classifier, {\n",
    "    X: training_examples\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.006181810554592e-06"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-entropy loss\n",
    "\n",
    "C = Placeholder()\n",
    "\n",
    "# TODO: not sure about the ReduceSum over all dimensions here\n",
    "cross_entropy_loss = Negate(ReduceSum(\n",
    "    Multiply(C, Log(classifier))\n",
    "))\n",
    "\n",
    "Session().run(cross_entropy_loss, {\n",
    "    X: training_examples,\n",
    "    C: labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Optimizer\n",
    "\n",
    "class GradientDescentOptimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def minimize(self, loss):\n",
    "        learning_rate = self.learning_rate\n",
    "\n",
    "        class Minimize(Operation):\n",
    "            def __init__(self):\n",
    "                super().__init__(input_nodes=[loss])\n",
    "            \n",
    "            # TODO: this loss argument to compute is unneeded, because we just want to hook into the graph\n",
    "            def compute(self, loss):\n",
    "                # TODO: how to inject compute_gradients or just the gradients at each step?\n",
    "                loss_node = self.input_nodes[0]\n",
    "                gradients_table = compute_gradients(loss_node)\n",
    "\n",
    "#                 print('gradients_table: {}'.format(gradients_table))\n",
    "                \n",
    "                for node, gradient in gradients_table.items():\n",
    "                    if type(node) == Variable:\n",
    "                        node.value -= learning_rate * gradient\n",
    "        \n",
    "        return Minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards pass of Backpropagation which computes the gradients\n",
    "\n",
    "from queue import Queue\n",
    "\n",
    "def compute_gradients(loss):\n",
    "    gradients_table = {}\n",
    "    \n",
    "    # the loss gradient with regard to itself is 1\n",
    "    gradients_table[loss] = 1\n",
    "    \n",
    "    visited = set()\n",
    "    queue = Queue()\n",
    "    queue.put(loss)\n",
    "    visited.add(loss)\n",
    "    \n",
    "    # BFS from loss backwards to inputs\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "\n",
    "#         if hasattr(node, 'consumers'):\n",
    "        if node != loss: # the loss doesn't have any consumers\n",
    "            # compute gradient of node\n",
    "\n",
    "            # stores gradient of the loss wrt the node's output\n",
    "            gradients_table[node] = 0\n",
    "            \n",
    "#             print('node: {}'.format(node))\n",
    "#             print('node.input_nodes: {}'.format(node.input_nodes))\n",
    "            \n",
    "            for consumer in node.consumers:\n",
    "                # get the accumulated gradient for the consumer\n",
    "                loss_gradient_wrt_consumer_output = gradients_table[consumer]\n",
    "                \n",
    "                # TODO: use multiplication for the chain rule and do not pass gradients around\n",
    "                # actually it is not so trivial to do that\n",
    "                \n",
    "#                 print('consumer: {}'.format(consumer))\n",
    "                \n",
    "                # apply the chain rule, using consumer's accumulated gradient, to compute...\n",
    "                # TODO: consumer.gradient or node.gradient?\n",
    "                loss_gradient_wrt_consumer_inputs = consumer.gradient(loss_gradient_wrt_consumer_output)\n",
    "                \n",
    "#                 print('loss_gradient_wrt_consumer_inputs: {}'.format(loss_gradient_wrt_consumer_inputs))\n",
    "                \n",
    "                if len(consumer.input_nodes) == 1:\n",
    "#                     print('trying to add {} to gradient {}'.format(loss_gradient_wrt_consumer_inputs, gradients_table[node]))\n",
    "                    # the consumer has only 1 input (the current node), thus consumer's input is scalar\n",
    "                    gradients_table[node] += loss_gradient_wrt_consumer_inputs\n",
    "                else:\n",
    "                    # otherwise the consumer consumes a vector\n",
    "                    # find the index of node in consumer's inputs\n",
    "                    node_index_in_consumer_inputs = consumer.input_nodes.index(node)\n",
    "                    \n",
    "                    # get the gradient only from the edge from node to consumer\n",
    "                    loss_gradient_wrt_node = loss_gradient_wrt_consumer_inputs[node_index_in_consumer_inputs]\n",
    "                    \n",
    "                    # accumulate the gradient for that consumer\n",
    "                    gradients_table[node] += loss_gradient_wrt_node\n",
    "\n",
    "        # continue backwards to node's inputs\n",
    "        if hasattr(node, 'input_nodes'):\n",
    "            for input_node in node.input_nodes:\n",
    "                # Placeholder nodes do not have a gradient, so ignore them\n",
    "                if not input_node in visited and not type(input_node) == Placeholder:\n",
    "                    queue.put(input_node)\n",
    "                    visited.add(input_node)\n",
    "    \n",
    "    return gradients_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 8.26458938989594\n",
      "Step: 10  Loss: 0.1652251421349391\n",
      "Step: 20  Loss: 0.08367745749183048\n",
      "Step: 30  Loss: 0.05668636296948762\n",
      "Step: 40  Loss: 0.04309110803322651\n",
      "Step: 50  Loss: 0.03486166620694781\n",
      "Step: 60  Loss: 0.029328832181125745\n",
      "Step: 70  Loss: 0.025346073543178964\n",
      "Step: 80  Loss: 0.02233789199912719\n",
      "Step: 90  Loss: 0.019983150161384144\n",
      "Weight matrix:\n",
      " [[ 0.87961879 -0.52116893]\n",
      " [ 1.20170703  0.96901137]]\n",
      "Bias:\n",
      " [-0.23746006 -0.23083027]\n"
     ]
    }
   ],
   "source": [
    "# Multi-class Perceptron\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "graph = Graph().as_default()\n",
    "\n",
    "training_examples = np.array([\n",
    "    [-3, -3],\n",
    "    [-3, -4],\n",
    "    [4, 5],\n",
    "    [3, 6]\n",
    "])\n",
    "\n",
    "labels = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "X = Placeholder()\n",
    "C = Placeholder()\n",
    "\n",
    "# W = Variable(np.array([\n",
    "#     [1, -1],\n",
    "#     [1, -1]\n",
    "# ]))\n",
    "# b = Variable(np.array([0, 0]))\n",
    "\n",
    "# Initialize weights randomly\n",
    "W = Variable(np.random.randn(2, 2))\n",
    "b = Variable(np.random.randn(2))\n",
    "\n",
    "\n",
    "perceptron_classifier = Softmax(Add(Matmul(X, W), b))\n",
    "\n",
    "# TODO: not sure about the ReduceSum over all dimensions here\n",
    "cross_entropy_loss = Negate(ReduceSum(\n",
    "    Multiply(C, Log(perceptron_classifier))\n",
    "))\n",
    "\n",
    "# Build minimization operation\n",
    "minimization_op = GradientDescentOptimizer(learning_rate=0.01).minimize(cross_entropy_loss)\n",
    "\n",
    "# Build placeholder inputs\n",
    "feed_dict = {\n",
    "    X: training_examples,\n",
    "    C: labels\n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "\n",
    "# Perform 100 gradient descent steps\n",
    "for step in range(100):\n",
    "    loss_value = session.run(cross_entropy_loss, feed_dict)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Step:\", step, \" Loss:\", loss_value)\n",
    "    session.run(minimization_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "W_value = session.run(W)\n",
    "print(\"Weight matrix:\\n\", W_value)\n",
    "b_value = session.run(b)\n",
    "print(\"Bias:\\n\", b_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
